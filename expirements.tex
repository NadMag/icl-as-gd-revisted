\subsection{Evaluation Datasets}

We evaluated our experiments on six datasets. \textbf{SST2} \cite{socher-etal-2013-recursive} \textbf{SST5} \cite{socher-etal-2013-recursive}, \textbf{MR} \cite{10.3115/1219840.1219855} and \textbf{Subj} \cite{10.3115/1218955.1218990} are four datasets for sentiment classification; \textbf{AGNews} \cite{NIPS2015_250cf8b5} is a topic classification dataset; and \textbf{CB} \cite{Marneffe2019TheCI} is used
for natural language inference.

\subsection{Evaluation Metrics}

In the following sections we describe the evaluation metrics used to compare the behavior of ICL and finetuning.
To ensure an optimal comparison, we have adopted the identical metrics as introduced in \cite{dai2023gpt}:
We design three metrics to measure the similarity between ICL and finetuning at three different levels: the prediction level, the representation level, and the attention behavior level. 

\paragraph{Prediction Recall}

From the perspective of model prediction, models with similar behavior should have aligned predictions.
We measure the recall of correct ICL predictions to correct finetuning predictions.
Given a set of test examples, we count the subsets of examples correctly predicted by each model: $C_{\text{ZSL}}, C_{\text{ICL}}, C_{\text{FT}}$.
To compare the update each method induces to the model's prediction we subtract correct predictions made in the ZSL setting.
Finally we compute the \textbf{Rec2FTP} score as: $\frac{ \sizeof{ \left( C_{\text{ICL}} \cap C_{\text{FT}} \right) \setminus C_{\text{ZSL}} } }{ \sizeof{ C_{\text{FT}} \setminus C_{\text{ZSL}} } }$ .
A higher Rec2FTP score suggests that ICL covers more correct behavior of finetuning from the perspective of the model prediction.

%This measure is agnostic to the inner workings of the attention mechanism.

\paragraph{Attention Output Direction}
In the context of an attention layer's hidden state representation space within a model, we analyze the modifications made to the attention output representation (\textbf{SimAOU}).

For a given query example, let $h^{(l)}_X$ represent the normalized output representation of the last token at the $l$-th attention layer within setting $X$. The alterations introduced by ICL and finetuning in comparison to ZSL are denoted as $h^{(l)}_{ICL} - h^{(l)}_{ZSL}$ and $h^{(l)}{FT} - h^{(l)}_{ZSL}$, respectively. We calculate the cosine similarity between these two modifications to obtain SimAOU ($\Delta FT$) at the $l$-th layer. A higher SimAOU ($\Delta FT$) indicates that ICL is more inclined to adjust the attention output in the same direction as finetuning.
For the sake of comparison, we also compute a baseline metric known as SimAOU (Random $\Delta$), which measures the similarity between ICL updates and updates generated randomly.

\paragraph{Attention Map Similarity}
We use SimAM to measure the similarity between attention maps and query tokens for ICL and finetuning.
For a query example, let $m^{(l,h)}_X$ represent the attention weights before softmax in the $h$-th head of the $l$-th layer for setting $X$. In ICL, we focus solely on query token attention weights, excluding demonstration tokens. Initially, before finetuning, we compute the cosine similarity between $m^{(l,h)}_{ICL}$ and $m^{(l,h)}_{ZSL}$, averaging it across attention heads to obtain SimAM (Before Finetuning) for each layer.
Similarly, after finetuning, we calculate the cosine similarity between $m^{(l,h)}_{ICL}$ and $m^{(l,h)}_{FT}$ to obtain SimAM (After FT). A higher SimAM (After FT) relative to SimAM (Before FT) indicates that ICL's attention behavior aligns more with a finetuned model than a non-finetuned one.


\subsection{Prediction Alignment}
In this section we focus on the perspective of model predictions, regarding both ICL and finetuning as black-box updates to the zero-shot setting prediction.
While this analysis provides less insight into the inner workings of ICL, prediction alignment is easily interpretable and seems necessary for downstream applications of such comparisons.

We revisit the results of \cite{dai2023gpt}, and note the discrepancy between the accuracy of the finetuned model and the ICL setting.
Our evaluation shows an average relative difference of $19.38\%$ between ICL and finetuning accuracy with respect to the original model.
To better understand .... we measure .... reference fig

Specifically \cite{dai2023gpt} find that ICL achieves high \textbf{Rec2FTP} scores, which means it covers most of the correct predictions of finetuning.
%relative diff in acc by task (1_3B)[0.2666666666666666, 0.13994910941475827, 0.24279210925644915, 0.16804407713498629, 0.30021598272138245, 0.045333333333333406]$%
We argue that this metric is insufficient because (1) it does not account for the accuracy discrepancy, (2) it does not reflect incorrect prediction updates which may reflect meaningful behavioral changes in the model.


To address these fallacies we propose a measurement of \textbf{relative prediction alignment} (RPA) to quantify the similarity between both prediction updates.
Given a validation set, we denote the subset of example whose prediction is changed with regards to the zero-shot setting by ICL or FT by $D_\text{ICL}$ and $D_\text{FT}$ respectively.
We define the RPA of these updates by: $\frac{\sizeof{D_\text{ICL} \cap D_\text{FT}}}{\sizeof{D_\text{ICL}} + \sizeof{D_\text{FT}} - \sizeof{D_\text{ICL} \cap D_\text{FT}}}$.     


\input{}