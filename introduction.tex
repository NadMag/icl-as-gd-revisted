In recent studies \cite{pmlr-v202-von-oswald23a,dai2023gpt}, attempts have been made to establish a link between in-context learning (ICL) and the fine-tuning process using gradient descent (GD) in transformer models. One common conclusion that researchers frequently derive from these studies is that "ICL essentially enacts gradient descent." Nevertheless, it's worth noting that much of the analysis in these investigations focused on models with linear attention, which can be considered as a simplified setting.

In their study, \cite{dai2023gpt} expand their findings from linear attention to conventional attention mechanisms, relying on empirical evidence to support their assertions.
Their experiments convincingly demonstrate that a model fine-tuned through gradient descent steps and a model prompted with in-context examples appear to execute similar functions. In simpler terms, they exhibit analogous behaviors when processing inputs. Furthermore, they observe a substantial overlap in the internal behaviors of these two models.
Yet, it remains unclear how a transformer's forward pass calculates its backward pass, even if it's done in a clever way involving a smaller transformer embedded within the weights of the original transformer, as suggested by some. Even if this is theoretically achievable, it's a complex concept to put into practice. In contrast, when it comes to linear models, the gradient step has a straightforward mathematical formula, making it much simpler to handle.

In \cite{pmlr-v202-von-oswald23a}, they put forth a less ambitious idea. They suggest that the model essentially carries out a kind of gradient descent on a simple linear model applied to the original deep representations calculated during the forward pass.
Using linear models with detailed feature representations is known to be highly effective in accomplishing various tasks.


In this study, our goal is to demonstrate that this principle holds true in this context as well. This can support our argument that extensive fine-tuning is not required, and a basic linear model that uses the original hidden states as inputs suffices.
To do so, we perform three experiments:
\begin{itemize}
    \item \textbf{Linearization}: We replicated the similarities results of \cite{dai2023gpt} and compared them with a simplified version the function we optimize by linearization.
    \item \textbf{Gilad's experiment}: \textbf{TODO!!}
    \item \textbf{Labels Switching}: To further confirm the results of \cite{dai2023gpt}, motivated by textbf{TODO: add relevant labels experiments papers}, we evaluated the change in similarity when providing false labels both in FT and ICL.
\end{itemize}
