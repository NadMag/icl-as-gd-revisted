In recent years, large language models \cite{brown20gpt3} have shown strong emergent in-context learning ability \cite{wei2022emergent} where a pretrained model's performance significantly improves on various downstream tasks by simply conditioning on a few input-label pairs (demonstrations).
Despite its success and avid research regarding ICL abilities, the inner workings behind the process are still not fully understood.
In-context learning operates in a seemingly different approach to few-shot and meta-learning which require additional parameter updates.
Nevertheless a series of recent works show significant similarities between ICL and gradient descent based optimization \cite{irie22dual, pmlr-v202-von-oswald23a, aky√ºrek2023learning}.  

In this paper, we study the results of \cite{dai2023gpt} which empirically show connections between ICL and standard finetuning on large GPT models and language classification tasks.
We identify both empirical differences in previous results such as a discrepancy in accuracy, and intrinsic theoretical differences in layer causality and update expressiveness.
Our goal is to quantify and demonstrate how these factors affect the results of the comparison between the two methods.
To do so, we perform experiments on three levels:
\begin{itemize}
    \item \textbf{Prediction Alignment}: From the model prediction perspective, we demonstrate that ICL's and finetuning's predictions differ on most tasks. 
    We introduce a new metric: relative prediction alignment (RPA) to better measure similarity with ICL in this setting.
    \item \textbf{Layer Causality}: Focusing on the flow of information in the model, we identify a that the update induced to each layers prediction in finetuning depends on all the others (no layer causality).
    To study how this difference affects the similarity with ICL, we propose a new layer causal finetuning method and evaluate its similarity with ICL.
    \item \textbf{Update Expressiveness}: Inspired by recent studies large language models from the neural tangent kernel (NTK) perspective \cite{linearization23},
    we test whether comparable similarities with ICL could be achieved by performing finetuning on a less expressive model, namely the model's linear approximation.
\end{itemize}
