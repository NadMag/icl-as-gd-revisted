\pdfoutput=1

\documentclass[11pt]{article}

% \usepackage[review]{ACL2023}
\usepackage{ACL2023}

\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{hyperref}
% \RequirePackage{algorithm}
% \RequirePackage{algorithmic}

\input{settings.tex}
\input{math_commands.tex}

% \newcommand{\cmark}{\ding{51}\xspace}%
\newcommand{\cmarkg}{\textcolor{lightgray}{\ding{51}}\xspace}%
% \newcommand{\xmark}{\ding{55}\xspace}%
\newcommand{\xmarkg}{\textcolor{lightgray}{\ding{55}}\xspace}%

\newcommand\our{\textsc{structured prompting}}
\newcommand{\tblidx}[1]{{\scriptsize \texttt{[#1]}}}
\newtheorem{theorem}{Property}

\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{{\color{blue}\ding{51}}}%
\newcommand{\xmark}{{\color{black}\ding{55}}}%


\title{Understanding In-Context Learning in Large Language Models as Gradient Descent Revisted}


\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}

\author{\\
Tomer Bar Natan,~Gilad Deutch,~Nadav Magar\\
~~~~Supervised by Guy Dar\\
~~~~~Tel-Aviv University}

\date{}

\begin{document}

\maketitle

\begin{abstract}
	In-context learning (ICL) has shown impressive results in few-shot learning tasks, yet the underlying mechanism of it is still not fully understood.
	Recent research suggests that ICL is similar to gradient descent (GD)-based fine-tuning.
	Mathematical analysis shows an equivalence between ICL to gradient descent on a transformer with linear attention and one layer,
	yet empirical research has shown only limited success in showing equivalence on non-linear, multi-layered models.
	In this work, we experiment with a new training process that we call per-layer training,
	and provide some empricial evidence to suggest that this process may explain the underlying mechanism of ICL better than standard fine-tuning.
	Per-layer training, we beileve, aligns better with the mathematical analysis of the ICL and GD equivalence.
	We also experiment with fine-tuning the model with linear approximation of it.
	% [git]
	Code implementation for all our experiments can be found here:
	\href{https://github.com/GiilDe/ft-vs-icl}{https://github.com/GiilDe/ft-vs-icl}
\end{abstract}

\section{Introduction}
\input{introduction.tex}

\section{Background and Preliminaries}
\input{background.tex}


\section{Experiments}
\subsection{Per-Layer Training}
There are 2 points in which the analysis in sections \ref{sec:icl_dual_1} and \ref{sec:icl_dual} differs from the standard fine-tuning process:

\begin{enumerate}
	\item Direction of "information flow": in standard model training, due to the nature of backpropagation, the gradient flows from the output layer to the input layer.
	      This is the opposite direction of information flow in the ICL process, where each layer is ignorant of the output of the subsequent layers.
	      In the per-layer training, each layer's gradient does not depend on later layers.
	      In \cite{dai2023gpt}, the authors show that the similarity between ICL and finetuning is higher for the final layers of the model, suggesting that our intuition about the direction of information flow may be correct.
	\item The analysis of the similarity between ICL and fine-tuning, as shown in equations \ref{equ:icl_opti_dual} and \ref{equ:sgd_attn_dual} is done per-layer and is not expanded to the entire model.
\end{enumerate}
These deviations, lead us to fine-tune the GPT model with two changes to standard training: (1) we feed the output of each attention layer to the projection head and subsequently compute the cross entropy loss on it
(2) we detach the output of each layer from the computational graph, meaning that each layer does not propagate the gradient back to previous layers. Figure~\ref{per-layer-training} illustrates the process.



\begin{figure*}%
	\centering
	\subfloat{{\includegraphics[width=8cm]{per layer training.png}}}%
	\caption{Per-layer training. The output of each layer is fed to the projection head and the loss is computed on it. The losses are then summed to create the loss of a single training step.}
	\label{per-layer-training}
\end{figure*}

% \subsection{Tasks and Datasets}

% % [intro and statistics to 6 datasets]
% We compare ICL and finetuning based on six datasets spanning three classification tasks. 
% \textbf{SST-2}~\citep{sst}, \textbf{SST-5}~\citep{sst}, \textbf{MR}~\citep{mr} and \textbf{Subj}~\citep{subj} are four datasets for sentiment classification; 
% \textbf{AGNews}~\citep{dbpedia:agnews} is a topic classification dataset; 
% and \textbf{CB}~\citep{cb} is used for natural language inference. 
% The statistics of the validation examples and label types are summarized in Table~\ref{tab:dataset}. 

% \subsection{Experimental Settings}

% In our experiments, we use two GPT-like pretrained language models with 1.3B and 2.7B model parameters, respectively, which are released by fairseq\footnote{\url{https://github.com/facebookresearch/fairseq}}. 
% In the rest of this paper, we call them GPT 1.3B and GPT 2.7B for short. 
% All experiments are conducted on NVIDIA V100 GPUs with 32 GB memory. 

% For each task, we use the same template to format examples for Zero-Shot Learning~(ZSL), ICL, and finetuning. 
% Details of the templates used for each task are provided in Appendix~\ref{appendix:template}. 
% The answer prediction processes for ZSL and finetuning are the same with ICL as described in Section~\ref{sec:bg_icl}, except that they do not have demonstration examples. 

% For ICL, we fix the number of demonstration examples to 32 and tune the random seed for each task to find a set of demonstration examples that achieves the best validation performance. 
% For finetuning, we use the same demonstration examples for ICL as the training examples and use SGD as the optimizer. 
% For a fair comparison, we fine-tune the model for only one epoch and the 
% training examples are provided in the same order as demonstrated for ICL. 
% We tune the learning rate for finetuning and select the one that achieves the best validation performance. 
% Details of the search range and selected value for the random seeds and learning rates are shown in Appendix~\ref{appendix:hyper}. 
\input{expirements.tex}

\section{Results}
Due to limitation in compute resources, we ran all of the experiments with pretrained GPT model with 1.3 billion model parameters (GPT 1.3B), Hence all of the results described in this section are on that model
\begin{table*}[t]
	% \subfloat[original]{
	% 	\begin{tabular}{|l|r|r|r|r|r|r|r|}
	% 		\hline \textbf{Metric $\textbackslash$ Task} & SST2   & SST5   & MR    & Subj   & AGNews & CB     & Average \\
	% 		\hline Sim AUO Random                        & 0.0017 & 0.0029 & 0.001 & 0.0025 & 0.0021 & 0.0037 & 0.0023  \\
	% 		\hline Sim AUO FT                            & 0.1091 & 0.113  & 0.219 & 0.193  & 0.3053 & 0.2013 & 0.1901  \\
	% 		\hline SimAM Before FT                       & 0.5547 & 0.3914 & 0.398 & 0.378  & 0.1516 & 0.1524 & 0.3376  \\
	% 		\hline SimAM After FT                        & 0.585  & 0.4047 & 0.498 & 0.487  & 0.4944 & 0.1875 & 0.4427  \\
	% 		\hline
	% 	\end{tabular}
	% }
	% \quad
	% \subfloat[per-layer]{
	% 	\begin{tabular}{|l|r|r|r|r|r|r|r|r|}
	% 		\hline \textbf{Metric $\textbackslash$ Task} & SST2   & SST5   & MR     & Subj   & AGNews & CB     & Average \\
	% 		\hline Sim AUO Random                        & 0.0016 & 0.0025 & 0.0008 & 0.0022 & 0.0021 & 0.0037 & 0.0021  \\
	% 		\hline Sim AUO FT                            & 0.2297 & 0.1065 & 0.3299 & 0.3439 & 0.3213 & 0.3435 & 0.2791  \\
	% 		\hline SimAM Before FT                       & 0.5546 & 0.3913 & 0.3979 & 0.3786 & 0.1518 & 0.1524 & 0.3377  \\
	% 		\hline SimAM After FT                        & 0.5774 & 0.4039 & 0.2919 & 0.2844 & 0.1201 & 0.0293 & 0.2845  \\
	% 		\hline
	% 	\end{tabular}
	% }

	\centering
	\subfloat{
		\begin{tabular}{|l|r|r|r|r|r|r|r|}
			\hline \textbf{Metric $\textbackslash$ Task} & SST2   & SST5   & MR     & Subj   & AGNews & CB     & Average         \\
			\hline SimAOU Random                         & 0.0016 & 0.0025 & 0.0008 & 0.0022 & 0.0021 & 0.0037 & 0.0021          \\
			\hline SimAOU FT                             & 0.1091 & 0.113  & 0.219  & 0.193  & 0.3053 & 0.2013 & 0.1901          \\
			\hline SimAOU Per-Layer FT                   & 0.2297 & 0.1065 & 0.3299 & 0.3439 & 0.3213 & 0.3435 & \textbf{0.2791} \\
			\hline
		\end{tabular}
	}
	\quad
	\subfloat{
		\begin{tabular}{|l|r|r|r|r|r|r|r|}
			\hline \textbf{Metric $\textbackslash$ Task} & SST2   & SST5   & MR     & Subj   & AGNews & CB     & Average         \\
			\hline SimAM                                 & 0.5546 & 0.3913 & 0.3979 & 0.3786 & 0.1518 & 0.1524 & 0.3377          \\
			\hline SimAM FT                              & 0.585  & 0.4047 & 0.498  & 0.487  & 0.4944 & 0.1875 & \textbf{0.4427} \\
			\hline SimAM Per-Layer FT                    & 0.5774 & 0.4039 & 0.2919 & 0.2844 & 0.1201 & 0.0293 & 0.2845          \\
			\hline
		\end{tabular}
	}
	\caption{SimAOU and SimAM on all 6 datasets, comparing similarity between baseline metrics, standard finetuning and per-layer fine-tuning.}
	\label{tab:per_layer_metrics}
\end{table*}

\subsection{Per-Layer Training}
We recreate the experiments of \cite{dai2023gpt} using the per-layer training method.
The results are shown in Table~\ref{tab:per_layer_metrics}.
The SimAOU metric is higher for the per-layer training, but the SimAM metric is lower, and even lower than without fine-tuning for most tasks.

We hypothesize, at this point, that the reason for the lower SimAM metric is the gradient norm size.
The first layers of the model are now trained on a new objective, i.e their output is projected to the vocabulary space and the cross-entropy loss is computed on it instead of being passed for the next numerous layers.
Because of that, the gradient norm may be larger than the gradient norm of the standard training process.
We verify this hypothesis by measuring the norm of the gradients of each attention layer, during the standard training process and the per-layer training process.
The results are shown in Figure \ref{grad_norm}.

Following this finding, we attempt to apply gradient clipping to the per-layer training process with limited success.
We experiment with different clipping values for one task (Subj) and are able to improve the metrics for this task, see Table~\ref{tab:per_layer_clipping_metrics}.
Yet, we find that each task requires a different clipping value, and aim to find a more principled approach.
We attempt to clip the gradients by normalizing them to allow a maximum norm that is the norm measured in the standard fine-tuning, but this does not improve the results.


\begin{figure*}%
	\centering
	\subfloat{{\includegraphics[width=6cm]{per_layer_grad_norm (1).png}}}%
	\subfloat{{\includegraphics[width=6cm]{full_ft_grad_norm.png}}}%
	\caption{The gradient norm of each attention layer during the per-layer training process (left) and the standard fine-tuning process (right). The scale of the y-axis is different for each plot.}
	\label{grad_norm}
\end{figure*}


\begin{table*}[t]
	% \subfloat[Per-layer]{
	% 	\begin{tabular}{|l|r|r|r|r|r|r|r|}
	% 		\hline \textbf{Metric} & Subj   \\
	% 		\hline Sim AUO Random  & 0.0022 \\
	% 		\hline Sim AUO FT      & 0.3439 \\
	% 		\hline SimAM Before FT & 0.3786 \\
	% 		\hline SimAM After FT  & 0.2844  \\
	% 		\hline
	% 	\end{tabular}
	% }
	% \quad
	\centering
	\subfloat{
		\begin{tabular}{|l|r|}
			\hline \textbf{Metric $\textbackslash$ Task} & Subj   \\
			\hline Sim AUO Random                        & 0.0022 \\
			\hline Sim AUO FT                            & 0.348  \\
			\hline SimAM                                 & 0.3786 \\
			\hline SimAM After FT                        & 0.4227 \\
			\hline
		\end{tabular}
	}
	% \quad
	% \subfloat[Standard fine-tuning]{
	% 	\begin{tabular}{|l|r|}
	% 		\hline \textbf{Metric} & Subj   \\
	% 		\hline Sim AUO Random  & 0.0022 \\
	% 		\hline Sim AUO FT      & 0.193 \\
	% 		\hline SimAM Before FT & 0.3786 \\
	% 		\hline SimAM After FT  & 0.487 \\
	% 		\hline
	% 	\end{tabular}
	% }
	\caption{
		% Left shows the results for the per-layer training process. Middle shows the results for the per-layer training process with gradient norm clipping. Right shows the results for the standard fine-tuning process.
		Results for the per-layer training process with gradient norm clipping of max norm equal to 12.0.
		Compared to the results in Table~\ref{tab:per_layer_metrics}, the SimAM metric is higher than without the clipping, but not higher the standard fine-tuning process.
	}
	\label{tab:per_layer_clipping_metrics}
\end{table*}

% We first show the validation accuracy in the ZSL, ICL, and finetuning settings on six classification datasets in Table~\ref{tab:acc}. 
% Compared with ZSL, ICL and finetuning both achieve considerable improvements, which means the optimizations they make are both helpful to these downstream tasks. 
% In addition, we find that ICL is better at few-shot scenarios than finetuning. 

% \paragraph{Rec2FTP}
% We show the Rec2FTP scores for two GPT models on six datasets in Table~\ref{tab:similarity}. 
% As shown in the table, on average, ICL can correctly predict 87.64\% of the examples that finetuning can correct from ZSL. 
% These results indicate that at the prediction level, ICL can cover most of the correct behavior of finetuning. 

% \paragraph{SimAOU}
% We present the SimAOU scores averaged across examples and layers for two GPT models on six datasets in Table~\ref{tab:similarity}. 
% For comparison, we also provide a baseline metric~(\textbf{\textit{Random SimAOU}}) that computes the similarity between ICL updates and randomly generated updates. 
% From the table, we find that ICL updates are much more similar to finetuning updates than to random updates, which means at the representation level, ICL tends to change the attention results in the same direction as finetuning changes. 

% \paragraph{SimAM}
% Table~\ref{tab:similarity} also demonstrates the SimAM scores averaged across examples and layers for two GPT models on six datasets. 
% As a baseline metric for SimAM, \textbf{\textit{ZSL SimAM}} computes the similarity between ICL attention weights and ZSL attention weights. 
% Comparing these two metrics, we also observe that compared with ZSL, ICL is more inclined to generate attention weights similar to those of finetuning. 
% Again, at the attention behavior level, we prove that ICL behaves similarly to finetuning. 
\subsection{Linearization}
\input{linearization_results.tex}
\section{Discussions}
\subsection{Existence of an optimization process similar to ICL}
The mathematical analysis in Section~\ref{sec:icl_dual} suggests a similarity between ICL and finetuning.
Yet, it's important to note its limitations: (1) it is done on a linear attention layer (2) it is done on a single layer
(3) It is unclear what is the loss function for the optimization process that it suggests.
This means that the analysis is not sufficient to prove that non-linear, full model ICL is indeed a form of finetuning;
Hence, whether such an optimization process (or any kind of process) exists is still an open question.

\section{Related Work}
In-context learning (ICL) is a machine learning approach where a model fine-tunes its knowledge and adapts its behavior based on specific contextual information or examples, allowing it to perform better on tasks related to that context.
It enables models to leverage domain-specific or task-specific knowledge without extensive retraining, making them more versatile and adaptable.
In their work, \cite{NEURIPS2020_1457c0d6} explores the remarkable ability of language models, particularly GPT-3, to learn and perform tasks with minimal examples, demonstrating their potential as versatile few-shot learners.
The authors showcase the models' impressive performance across a wide range of tasks and emphasize their capacity to generalize from limited data, highlighting the transformative impact of these models on various natural language processing applications.

In recent research, there has been a growing interest in understanding the relationship between two key concepts: in-context learning (ICL) and gradient descent (GD)-based fine-tuning, particularly in the context of transformer models (\cite{pmlr-v202-von-oswald23a,2022arXiv221210559D}).
This research seeks to uncover how ICL, which involves adapting and learning in specific contexts, can be effectively integrated with the iterative optimization process of GD, especially when fine-tuning transformer models.
However, the majority of the examination was on models that had relaxed constraints and featured linear attention mechanisms:
\begin{equation}
	LinearAttn(K,V,q)=KV^q
\end{equation}

The paper \cite{pmlr-v202-von-oswald23a}, develops an explicit weight values for a linear self-attention layer, achieving an update equivalent to a single iteration of gradient descent (GD) aimed at minimizing mean squared error. Moreover, the authors demonstrate how multiple self-attention layers can progressively execute curvature adjustments, leading to enhancements over standard gradient descent.
They proposed the following:

Given a 1-head linear attention layer and
the tokens $e_{j} = (x_{j},y_{j})$, for $j = 1, . . . , N$, one can construct key, query and value matrices $W_{K}$, $W_{Q}$, $W_{V}$ as well
as the projection matrix P such that a Transformer step on
every token $e_j$ is identical to the gradient-induced dynamics $e_j \rightarrow (x_j , y_j ) + (0, - \delta W x_j ) = (x_j , y_j ) + PVK^{T}q_j$
such that $e_j = (x_j , y_j - \delta y_j )$. For the test data token
$(x_{N+1}, y_{N+1})$ the dynamics are identical.

By doing so, they demonstrate the capability of linear attention to execute gradient descent on the deep representations constructed by the transformer.


Another paper (\cite{2022arXiv221210559D}) expand the findings from linear attention to conventional attention mechanisms, substantiating their claims with empirical data.
Inspired by \cite{Aizerman2019TheoreticalFO} and \cite{unknown}, the idea in this is paper to explain language models as meta-optimizers.

Consider $W_0$ and $\Delta W$, both belonging to $\mathbb{R}^{d_{out} \times d_{in}}$, where $W_0$ represents the initial parameter matrix, and $\Delta W$ signifies the updating matrix. Additionally, let $x$ be a member of $\mathbb{R}^{d_{in}}$, serving as the input representation. A linear layer, subject to optimization via gradient descent, can be articulated as follows:
\begin{equation}
	\mathcal{F}(x) = (W_0 + \Delta W)x \label{eq:2}
\end{equation}
In the context of the back-propagation algorithm, the determination of $\Delta W$ entails the aggregation of outer products derived from historical input representations $x'_i \in \mathbb{R}^{d_{in}}$ and their corresponding error signals $e_i \in \mathbb{R}^{d_{out}}$:
\begin{equation}
	\Delta W = \sum_{i} e_i \otimes x'_i \label{eq:3}
\end{equation}
Notably, $e_i$ is the result of scaling historical output gradients by $-\gamma$, the negative learning rate.

By equations \eqref{eq:2} and \eqref{eq:3}, we can derive the dual manifestation of linear layers, optimized through gradient descent, as follows:
\begin{align}
	\begin{split}
		\mathcal{F}(x) &= (W_0 + \Delta W)x \\
		&= W_0x + \Delta Wx \\
		&= W_0x + \sum_{i} (e_i \otimes x'_i)x \\
		&= W_0x + \sum_{i} e_i(x^{'T}_ix) \\
		&= W_0x + \text{LinearAttn}(E, X', x)
	\end{split}
	\label{eq:4}
\end{align}
Here, $E$ denotes historical output error signal values, $X'$ corresponds to historical inputs employed as keys, and $x$ serves as the current input, operating as the query.

Their experiments convincingly reveal that a model fine-tuned through gradient steps and a model prompted with in-context examples appear to perform analogous functions, exhibiting similar behaviors on inputs.
Additionally, they observe significant similarities in the internal behaviors of these two models.

% \section{Related Work}


% xxx

\section{Conclusion}
Following recent research that suggested a theoretical equivalency between in-context learning (ICL) and gradient descent (GD)-based fine-tuning, we've attempted to further explore this relationship in practical settings.
We experimented with a new training process that we call per-layer training,
and provided some empricial evidence to suggest that this process may explain the underlying mechanism of ICL better than standard fine-tuning.
Another experiment that we try is to fine-tune the model with linear approximation of it.
In our discussion section we discuss the limitations of our experiments, we also suggest that whether an optimization process similar to ICL exists is still an open question.
\section{Acknowledge}
\input{acknowledge.tex}
\nocite{rw1}
\nocite{rw2}

\bibliography{anthology,refs}
\bibliographystyle{acl_natbib}

\newpage
\appendix


\end{document}
